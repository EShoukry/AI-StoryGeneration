{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bidirectional LSTM for Story Generation",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EShoukry/AI-StoryGeneration/blob/master/Bidirectional_LSTM_for_Story_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jSMSF2huH54",
        "colab_type": "text"
      },
      "source": [
        "# Importing all the libraries needed, dictionaries, mounting the google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wospckGqwfRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "#import Keras library\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import LSTM, Input, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "import keras.callbacks \n",
        "from keras.metrics import categorical_accuracy\n",
        "\n",
        "#import spacy, and spacy french model\n",
        "# spacy is used to work on text\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "#import other libraries\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import codecs\n",
        "import collections\n",
        "from six.moves import cPickle\n",
        "seq_length = 30\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#define parameters used in the tutorial\n",
        "data_dir = 'drive/My Drive/Colab Notebooks/'# data directory containing raw texts\n",
        "save_dir = 'drive/My Drive/Colab Notebooks/' # directory to store trained NN models\n",
        "file_list = [\"MonsterManual\"]\n",
        "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
        "sequences_step = 1 #step to create sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe4dC8FlnGhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = 'drive/My Drive/Colab Notebooks/'# data directory containing raw texts\n",
        "save_dir = 'drive/My Drive/Colab Notebooks/' # directory to store trained NN models\n",
        "input_file = os.path.join(data_dir, file_name + \".txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLH8gOPWuxLC",
        "colab_type": "text"
      },
      "source": [
        "## Function to make a list of all the words from the text file made from the PDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM__E7Btx3rO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_wordlist(doc):\n",
        "    wl = []\n",
        "    for word in doc:\n",
        "        if word.text not in (\"\\n\",\"\\n\\n\",\"\\n\\n \",'\\u2009','\\xa0'):\n",
        "            wl.append(word.text.lower())\n",
        "    return wl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2lBDLSGx9tS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordlist = []\n",
        "for file_name in file_list:\n",
        "    input_file = os.path.join(data_dir, file_name + \".txt\")\n",
        "    #read data\n",
        "    with codecs.open(input_file, \"rb\", encoding = \"utf-8\",  errors='ignore') as f:\n",
        "        data = f.read(500000)\n",
        "    #create sentences\n",
        "    doc = nlp(data)\n",
        "    wl = create_wordlist(doc)\n",
        "    wordlist = wordlist + wl\n",
        "for word in wordlist:\n",
        "  if word.isspace:\n",
        "    wordlist.remove(word)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azYgOcsPxzSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sorted(wordlist))\n",
        "print(sorted(wordlist).index('a'))\n",
        "wordlist = sorted(wordlist)[24123:]\n",
        "print(wordlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwPEnKYZyLor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# count the number of words\n",
        "word_counts = collections.Counter(wordlist)\n",
        "\n",
        "# Mapping from index to word : that's the vocabulary\n",
        "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "vocabulary_inv = list(sorted(vocabulary_inv))\n",
        "\n",
        "# Mapping from word to index\n",
        "vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "words = [x[0] for x in word_counts.most_common()]\n",
        "\n",
        "#size of the vocabulary\n",
        "vocab_size = len(words)\n",
        "print(\"vocab size: \", vocab_size)\n",
        "\n",
        "#save the words and vocabulary\n",
        "with open(os.path.join(vocab_file), 'wb') as f:\n",
        "  cPickle.dump((words, vocab, vocabulary_inv), f)\n",
        "print(word_counts)\n",
        "print(vocabulary_inv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZW-F0OE9UOs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ded0caeb-11a8-4aa6-84ce-99387a646ee0"
      },
      "source": [
        "#create sequences\n",
        "seq_length = 30\n",
        "sequences = []\n",
        "next_words = []\n",
        "for i in range(0, len(wordlist) - seq_length, sequences_step):\n",
        "    sequences.append(wordlist[i: i + seq_length])\n",
        "    next_words.append(wordlist[i + seq_length])\n",
        "\n",
        "print('nb sequences:', len(sequences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb sequences: 36216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOhH79kQ9auG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "000617be-ea06-415f-89db-ce8f40597066"
      },
      "source": [
        "X = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), vocab_size), dtype=np.bool)\n",
        "print(\"Complete\")\n",
        "for i, sentence in enumerate(sequences):\n",
        "    for t, word in enumerate(sentence):\n",
        "        X[i, t, vocab[word]] = 1\n",
        "    y[i, vocab[next_words[i]]] = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVaK06Ji9r9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bidirectional_lstm_model(seq_length, vocab_size):\n",
        "    print('Build LSTM model.')\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(vocab_size))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    optimizer = Adam(lr=learning_rate)\n",
        "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
        "    print(\"model built!\")\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eytJNIpr9zVI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "a8f6ba2f-cf0b-489c-c5b9-f6ff56dd797b"
      },
      "source": [
        "rnn_size = 400 # size of RNN\n",
        "seq_length = 30 # sequence length\n",
        "learning_rate = 0.01 #learning rate\n",
        "\n",
        "md = bidirectional_lstm_model(seq_length, vocab_size)\n",
        "md.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build LSTM model.\n",
            "model built!\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_1 (Bidirection (None, 800)               20579200  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6030)              4830030   \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 6030)              0         \n",
            "=================================================================\n",
            "Total params: 25,409,230\n",
            "Trainable params: 25,409,230\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm9XzE8n91hB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32 # minibatch size\n",
        "num_epochs = 5 # number of epochs\n",
        "\n",
        "callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
        "           keras.callbacks.ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_gen_sentences.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
        "                           monitor='val_loss', verbose=0, mode='auto', period=2)]\n",
        "#fit the model\n",
        "history = md.fit(X, y,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True,\n",
        "                 epochs=num_epochs,\n",
        "                 callbacks=callbacks,\n",
        "                 validation_split=0.2)\n",
        "\n",
        "#save the model\n",
        "md.save(save_dir + \"/\" + 'finaltry.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8IMmtcQTySv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"loading vocabulary...\")\n",
        "vocab_file = os.path.join(save_dir, \"words_vocab.pkl\")\n",
        "\n",
        "with open(os.path.join(save_dir, 'words_vocab.pkl'), 'rb') as f:\n",
        "        words, vocab, vocabulary_inv = cPickle.load(f)\n",
        "\n",
        "vocab_size = len(words)\n",
        "\n",
        "from keras.models import load_model\n",
        "# load the model\n",
        "print(\"loading model...\")\n",
        "model = load_model(save_dir + \"/\" + 'finaltry.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYjZQR-_T5Qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjmCjJTAT-qR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "words_number = 50 # number of words to generate\n",
        "seed_sentences = \"the monster was giant\" #seed sentence to start the generating.\n",
        "\n",
        "#initiate sentences\n",
        "generated = ''\n",
        "sentence = []\n",
        "\n",
        "#we shate the seed accordingly to the neural netwrok needs:\n",
        "for i in range (seq_length):\n",
        "    sentence.append(\"a\")\n",
        "\n",
        "seed = seed_sentences.split()\n",
        "\n",
        "for i in range(len(seed)):\n",
        "    sentence[seq_length-i-1]=seed[len(seed)-i-1]\n",
        "\n",
        "generated += ' '.join(sentence)\n",
        "\n",
        "#the, we generate the text\n",
        "for i in range(words_number):\n",
        "    #create the vector\n",
        "    x = np.zeros((1, seq_length, vocab_size))\n",
        "    for t, word in enumerate(sentence):\n",
        "        x[0, t, vocab[word]] = 1.\n",
        "\n",
        "    #calculate next word\n",
        "    preds = model.predict(x, verbose=0)[0]\n",
        "    next_index = sample(preds, 10)\n",
        "    next_word = vocabulary_inv[next_index]\n",
        "\n",
        "    #add the next word to the text\n",
        "    generated += \" \" + next_word\n",
        "    # shift the sentence by one, and and the next word at its end\n",
        "    sentence = sentence[1:] + [next_word]\n",
        "\n",
        "#print the whole text\n",
        "print(generated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7Xl3j3jdPX7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}